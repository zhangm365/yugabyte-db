###########################################
#        Yugaware default configuration   #
###########################################
# You can override these in application.conf or through system properties.

# Install custom ObjectMapper
play.modules.disabled += "play.core.ObjectMapperModule"
play.modules.enabled  += "com.yugabyte.yw.modules.CustomObjectMapperModule"
play.modules.enabled  += "com.yugabyte.yw.modules.PerfAdvisorDBModule"


## Secret key
# http://www.playframework.com/documentation/latest/ApplicationSecret
# ~~~~~
# The secret key is used to sign Play's session cookie.
# This must be changed for production, but we don't recommend you change it in this file.
play.crypto.secret="changeme"
play.http.secret.key=${play.crypto.secret}

play.http.parser.maxDiskBuffer=100GB
play.forms.binding.directFieldAccess=true

kamon.instrumentation.logback.mdc.copy.enabled=false

akka {
  # On slow machines we sometimes see that logger is not able to start in 5 secs
  logger-startup-timeout=30s
  actor {
    default-dispatcher {
      type = "com.yugabyte.yw.common.logging.MDCPropagatingDispatcherConfigurator"
    }
  }
}

application.home = "."
log.override.path = ${application.home}"/logs"

# ============= START DATABASE RELATED CONFIGURATION ==================================

# Lets disable the default play evolutions and use flyway db (overridden by yugabyted)
play.evolutions.enabled=false
# We use our own flyway initializer see: com.yugabyte.yw.common.ybflyway.PlayInitializer
# So we do not need to enable flyway-play module
# play.modules.enabled += "org.flywaydb.play.PlayModule"

db {
  # Tracking default postgresql connection details
  default {
    host="localhost"
    port=5432
    dbname="yugaware"
    username="postgres"
    username=${?DB_USERNAME}
    password=""
    password=${?DB_PASSWORD}
    driver="org.postgresql.Driver"
    url="jdbc:postgresql://"${db.default.host}":"${db.default.port}"/"${db.default.dbname}
    logStatements=true
    pg_dump_path=""
    pg_restore_path=""
    # Config about flyway migrations.
    migration {
      table=schema_version
      initOnMigrate=true
      auto=true
      ignoreMigrationPatterns=["*:missing","*:future"]
      outOfOrder=true
      scriptsDirectory="default_"
      # We want to use postgres db in production
      # The migration scripts will be under resources/db.migration.default.postgres
      # with common scripts under resources/db.migration.default.common
      locations=["common","postgres"]
    }
  }
  perf_advisor {
    # We assume both databases are on the same PG instance. If not - this needs to be overriden
    username=${db.default.username}
    password=${db.default.password}
    url="jdbc:postgresql://"${db.default.host}":"${db.default.port}"/"${db.perf_advisor.dbname}
    createDatabaseUrl="jdbc:postgresql://"${db.default.host}":"${db.default.port}"/"${db.default.dbname}
    migration {
      ignoreMigrationPatterns=["*:missing","*:future"]
      table=schema_version
    }
  }
}

ebean {
  default = ["com.yugabyte.yw.*"]
}
# =============  END DATABASE RELATED CONFIGURATION ==================================

yb {
  url = ""
  mode="PLATFORM"
  seedData=false
  multiTenant=false
  use_kubectl=true
  use_new_helm_naming=true
  universe_version_check_mode=NEVER  # possible values: NEVER, HA_ONLY, ALWAYS
  universe_boot_script = ""
  is_platform_downgrade_allowed=false
  is_platform_downgrade_allowed=${?YB_IS_PLATFORM_DOWNGRADE_ALLOWED}
  start_master_on_stop_node=true
  use_spot_instances = false
  log.logEnvVars=false
  devops.home = ""
  thirdparty.packagePath = /opt/third-party
  grafana.accessKey="changeme"
  allow_db_version_more_than_yba_version=false
  kubernetesOperator.enabled=false
  fixPaths=false

  regex {
    release_pattern {
      ybdb = "[^.]+yugabyte-(?:ee-)?(.*)-(alma|centos|linux|el8|darwin)(.*).tar.gz"
      helm = "[^.]+yugabyte-(.*)-helm.tar.gz"
    }
  }
  docker {
    network = "bridge"
    release = ""
  }
  swamper {
    targetPath = ""
    rulesPath = ""
  }
  ui {
    enable_dedicated_nodes=true
    show_cost=true

    feature_flags {
      perf_advisor=true
      k8s_custom_resources=false
      provider_redesign=false
    }
  }

  edit_provider {
    new {
      enabled = true
    }
  }

  provider {
    skip_keypair_validation = false
    validate_ntp_server_count = 1
    allow_used_provider_edit = ${yb.cloud.enabled}
  }

  universe {
    auth {
      is_enforced = false
    }
    user_tags {
      is_enforced = false
      enforced_tags = []
    }
  }

  xcluster {
    k8s_tls_support = true
    transactional {
      enabled = false
    }
  }

  # Enable/Disable Runtime Config UI under Admin section
  runtime_conf_ui {
    enable_for_all = false
    tag_filter = ["PUBLIC"]
  }

  # Alerts thresholds
  alert {
    # Value of maximum allowed clock skew before an alert is generated (in ms).
    max_clock_skew_ms = 500
    # Value of maximum allowed replication lag before an alert is generated (in ms).
    replication_lag_ms = 180000
    # Value of maximum allowed percents of used memory on nodes.
    max_memory_cons_pct = 90
    # Alert rules configuration sync interval in seconds.
    config_sync_interval_sec = 60
    # Maximum allowed number of nodes with health check errors.
    health_check_nodes = 0
    # Maximum allowed number of nodes with inactive cronjob.
    inactive_cronjob_nodes = 0
    # For how long do we let the alert be in database after it has resolved
    resolved_retention_duration = 120 days
    # Value of average CPU usage which triggers warning alert
    max_cpu_usage_pct_warn = 90
    # Value of average CPU usage which triggers severe alert
    max_cpu_usage_pct_severe = 95
    # Value of node disk usage which triggers severe alert
    max_node_disk_usage_pct_severe = 70
    # Value of node file descriptors usage which triggers severe alert
    max_node_fd_usage_pct_severe = 70
    # Value of allowed OOM kills per 10 minutes before severe alert is triggered
    max_oom_kills_severe = 3
    # Value of allowed OOM kills per 10 minutes before warning alert is triggered
    max_oom_kills_warning = 1
    # Value of node certificate expiry in days which triggers severe alert
    max_node_cert_expiry_days_severe = 30
    # Value of encryption at rest config expiry in days which triggers severe alert
    max_enc_at_rest_config_expiry_days_severe = 3
    # Maximum average latency for YSQL operations
    max_ysql_opavg_latency = 10000
    # Maximum average latency for YCQL operations
    max_ycql_opavg_latency = 10000
    # Maximum P99 latency for YSQL operations
    max_ysql_p99_latency = 60000
    # Maximum P99 latency for YCQL operations
    max_ycql_p99_latency = 60000
    # Maximum number of YSQL connections
    max_ysql_connections = 300
    # Maximum number of YCQL connections
    max_ycql_connections = 1000
    # Maximum number of YEDIS connections
    max_yedis_connections = 1000
    # Maximum YSQL throughput
    max_ysql_throughput = 100000
    # Maximum YCQL throughput
    max_ycql_throughput = 100000
    # Underreplicated masters threshold which triggers severe alert
    underreplicated_masters_secs_severe = 900
    # Underreplicated tablet threshold which triggers severe alert
    underreplicated_tablets_secs_severe = 300
    # Leaderless tablet threshold which triggers severe alert
    leaderless_tablets_secs_severe = 300
    # Value of days to expiry for SSH keys which triggers severe alert
    ssh_key_config_expiry_days_severe = 30
    pagerduty {
      ws = ${play.ws}
    }
    slack {
      ws = ${play.ws}
    }
    webhook {
      ws = ${play.ws}
    }
  }
  # Used to skip certificates validation for the configure phase.
  # Possible values - ALL, HOSTNAME
  #(the latter is used for skipping validation of commonName and subjectAltName)
  tls.skip_cert_validation = ""
  commissioner {
    # initial and minimum number of threads used by commissioner
    core_threads = 50

    # max number of threads we will grow to if needed before starting to reject tasks
    max_threads = 200

    # duration for which thread pool will stay inflated before it shrinks back to core_threads
    thread_ttl = 1 minute

    # Frequency at which we check task progress
    progress_check_interval = 10 s

    # capacity of the thread pool queue
    queue_capacity = 1000
  }

  maintenance {
    # For how long do we let the maintenance window be in database after it has finished
    retention_duration = 1200 days
  }

  task {
    # initial and minimum number of threads used by each task
    core_threads = 1

    # max number of threads we will grow to if needed before starting to reject tasks
    max_threads = 10

    # duration for which thread pool will stay inflated before it shrinks back to core_threads
    thread_ttl = 1 minute

    # capacity of the thread pool queue
    queue_capacity = 1000

    # Whether overriding universe lock is allowed when force option is selected.
    # If it is disabled, force option will wait for the lock to be released.
    override_force_universe_lock = false

    # How long force lock should retry acquiring the universe's lock when
    # `override_force_universe_lock` is false. If no unit is selected, it will be in milliseconds.
    max_force_universe_lock_timeout = "1800s"
  }

  backup_task {
    # initial and minimum number of threads used by each backup task
    core_threads = 1

    # max number of threads we will grow to if needed before starting to reject tasks
    max_threads = 10

    # duration for which thread pool will stay inflated before it shrinks back to core_threads
    thread_ttl = 1 minute

    # capacity of the thread pool queue
    queue_capacity = 2000
  }

  import {
    # initial and minimum number of threads used by import controller
    core_threads = 1

    # max number of threads we will grow to if needed before starting to reject tasks
    max_threads = 200

    # duration for which thread pool will stay inflated before it shrinks back to core_threads
    thread_ttl = 1 minute

    # capacity of the thread pool queue
    queue_capacity = 1000
  }

  authtoken {
    # Expiry time of auth token in days
    token_expiry = 7 days
  }

  # We delete completed task info form database.
  # Following config is for that task Garbage collection:
  taskGC {
    # How frequently do we check for completed tasks in database
    gc_check_interval = 1 days

    # For how long do we let the task be in database after it has completed
    task_retention_duration = 120 days
  }

  # Config for backup Garbage collection
  backupGC {
    # backup GC schedule run
    gc_run_interval = 15 minutes
  }

  snapshot_schedule {
    run_interval = 5 minutes
  }

  snapshot_cleanup {
    delete_orphan_on_startup = true
  }

  aws {
    # default volume count for aws instance types with EBS Only storage info
    default_volume_count = 1

    # Default AWS instance type
    default_instance_type = "c5.4xlarge"

    # default volume size for aws instance types with EBS Only storage info
    default_volume_size_gb = 250

    # List of supported AMI arch
    supported_arch_types = ["x86_64", "arm64"]

    # List of supported AMI root device
    supported_root_device_type = ["ebs"]

    # List of supported AMI platform
    supported_platform = ["linux"]

    storage {
      # Default storage type
      default_storage_type = "GP3"

      # GP3 free PIOPS
      gp3_free_piops = 3000

      # GP3 free throughput in MiB/sec
      gp3_free_throughput = 125
    }
  }

  gcp {
    # default volume size for gcp instance types
    default_volume_size_gb = 375

    # Default GCP instance type
    default_instance_type = "n1-standard-1"

    storage {
      default_storage_type = "Persistent"
    }
  }

  azure {
    # default volume size for azure instance types
    default_volume_size_gb = 250

    # Default Azure instance type
    default_instance_type = "Standard_DS2_v2"

    storage {
      default_storage_type = "Premium_LRS"
    }
  }

  kubernetes {
    storageClass = ""
    pullSecretName = ""
    # default volume count for kubernetes
    default_volume_count = 1

    # default volume size for kubernetes instance types
    default_volume_size_gb = 100

    # Default kubernetes instance type
    default_instance_type = "small"

    # default memory in gb
    default_memory_size_gb = 7.5

    # min memory in gb
    min_memory_size_gb = 2

    # max memory in gb
    max_memory_size_gb = 128

    # default CPU cores
    default_cpu_cores = 4

    # min CPU cores
    min_cpu_cores = 1

    # max CPU cores
    max_cpu_cores = 32
  }

  pwdpolicy {
    default_min_length = 8
    default_min_uppercase = 1
    default_min_lowercase = 1
    default_min_digits = 1
    default_min_special_chars = 1
  }
  metrics {
    host="localhost"
    port="9090"
    # Ideally this should not contain path - so that we can build any url out of it,
    # but for backward compatibility we leave it as is and just replace /api/v1
    # with other path where needed
    url = "http://"${yb.metrics.host}":"${yb.metrics.port}/api/v1
    # Empty means - use url above without /api/v1 suffix
    external.url = ""
    management.enabled = true
    db_read_write_test = true
    # Scrape target configuration sync interval in seconds.
    config_sync_interval_sec = 60
    scrape_interval = "10s"
    collection_level="NORMAL"
    ui {
      topk {
        enable=true
      }
    }
  }
  # sets logging level for file and stdout logs
  logging {
    config="DEBUG"
    rollover_pattern = "yyyy-MM-dd"
    max_history = "30"
    search_timeout_secs = "60"
  }
  storage.path="/opt/yugabyte"
  upgrade {
    #  Allow for leader blacklisting during universe upgrades
    blacklist_leaders = true
    blacklist_leader_wait_time_ms = 60000
    max_follower_lag_threshold_ms = 60000
    vmImage = ${yb.cloud.enabled}
    allow_downgrades=false
    allow_downgrades=${?YB_UPGRADE_ALLOW_DOWNGRADES}
    single_connection_ysql_upgrade=false
    allow_upgrade_on_transit_universe = false
    promote_auto_flag = true
    promote_flags_forcefully = false
  }
  edit {
    wait_for_leaders_on_preferred=true
  }
  releases {
    path = "/opt/yugabyte/releases"
    num_releases_to_keep_default = 3
    num_releases_to_keep_cloud = 2
    download_helm_chart_http_timeout = 60s
  }
  ha {
    replication_schedule_enabled = false
    replication_frequency = 30 minutes
    prometheus_config_dir = "/prometheus_configs"
    num_backup_retention = 10
    logScriptOutput = false
    ws = ${play.ws}
    # Override this ws config in runtime_config at global level
    # Reference: https://github.com/playframework/play-ws/blob/main/play-ws-standalone/src/main/resources/reference.conf
    # Example:
#      {
#      ssl {
#         loose.acceptAnyCert = false
#         trustManager = {
#           stores += { # append to certs defined in play.ws
#               type = "PEM"
#               data = """-----BEGIN CERTIFICATE-----
# MIIDzTCCArWgAwIBAgIQCjeHZF5ftIwiTv0b7RQMPDANBgkqhkiG9w0BAQsFADBa
# ... You can use triple quoted string for multiline data ...
# -----END CERTIFICATE-----"""
#           }
#           stores += {
#             ... you can trust multiple certs ...
#           }
#        }
#      }
#    }
  }

  wait_for_server_timeout = 300000 ms

  wait_for_master_leader_timeout = 30000 ms
  # Timeout for proxy endpoint request of db node
  proxy_endpoint_timeout = 1 minute

  wait_for_lb_for_added_nodes = false

  health {
    max_num_parallel_checks = 25
    max_num_parallel_node_checks = 50
    # Email address to send alerts to at YugaByte.
    default_email = ""
    default_email = ${?YB_ALERTS_EMAIL}

    debug_email = false

    # Default timeout for establishing the SMTP connection, in msec.
    smtp_connection_timeout_ms = 30000
    # Default timeout for sending the mail messages, in msec.
    smtp_timeout_ms = 60000

    # Interval at which to check the status of every universe. Default: 5 minutes.
    check_interval_ms = 300000
    # Interval at which to store the status of every universe in DB. Default: 5 minutes.
    store_interval_ms = 300000
    # Interval at which to send a status report email. Default: 12 hours.
    status_interval_ms = 43200000
    logOutput = false
    nodeCheckTimeoutSec = 180

    trigger_api.enabled = ${yb.cloud.enabled}
  }

  fs_stateless {
    suppress_error = ${yb.cloud.enabled}
    // 10MB
    max_file_size_bytes = 10000000
    max_files_count_persist = 10000
    disable_sync_db_to_fs_startup = ${yb.cloud.enabled}
  }

  perf_advisor {
    # disabled by default - can enable for particular universe
    enabled = false
    # max number of threads to support parallel querying of nodes
    max_threads = 22
    # interval for perf advisor scheduler runs, in minutes
    scheduler_interval_mins = 5
    # Perf advisor scheduler universe batch size
    universe_batch_size = 5
    # default interval for perf advisor runs for the universe, in minutes.
    universe_frequency_mins = 10

    cleanup {
      gc_check_interval = 1 days
      # For how long do we let the recommendation be in database
      rec_retention_duration = 30 days
      # For how long do we let the PA runs be in database
      pa_run_retention_duration = 30 days
    }
  }

  security {
    enable_detailed_logs = false
    enable_auth_for_proxy_metrics = true
    use_oauth = false
    use_oauth = ${?USE_OAUTH}
    type = ""
    type = ${?YB_SECURITY_TYPE}
    clientID = ""
    clientID = ${?YB_OIDC_CLIENT_ID}
    secret = ""
    secret = ${?YB_OIDC_SECRET}
    discoveryURI = ""
    discoveryURI = ${?YB_OIDC_DISCOVERY_URI}
    oidcScope = ""
    oidcScope = ${?YB_OIDC_SCOPE}
    oidcEmailAttribute = ""
    oidcEmailAttribute = ${?YB_OIDC_EMAIL_ATTR}
    enable_external_script = false
    enable_external_script = ${?ENABLE_EXTERNAL_SCRIPT}
    ssh2_enabled = false
    ldap {
      use_ldap = "false"
      ldap_url = ""
      ldap_port = ""
      ldap_basedn = ""
      ldap_dn_prefix = "CN="
      ldap_customeruuid = ""
      ldap_service_account_username = ""
      ldap_service_account_password = ""
      enable_ldaps = false
      enable_ldap_start_tls = false
      use_search_and_bind = false
      ldap_search_attribute = ""
    }
    forbidden_ips="169.254.169.254"
    custom_hooks {
      enable_custom_hooks = false
      enable_sudo = false
      enable_api_triggered_hooks = ${yb.cloud.enabled}
    }
    ssh_keys {
      enable_ssh_key_expiration = true
      ssh_key_expiration_threshold_days = 365
    }
    default.access.key = "yugabyte-default"
  }

  ansible {

    # strategy can be linear, mitogen_linear or debug
    strategy = "linear"
    # https://docs.ansible.com/ansible/latest/reference_appendices/config.html#default-timeout
    conn_timeout_secs = 60

    # verbosity of ansible logs, 0 to 4 (more verbose)
    verbosity = 0
    # debug output (can include secrets in output)
    debug = false

    # https://docs.ansible.com/ansible/latest/reference_appendices/config.html#diff-always
    diff_always = false

    # https://docs.ansible.com/ansible/latest/reference_appendices/config.html#default-local-tmp
    local_temp = "/tmp/ansible_tmp/"

  }

  customer_task_db_query_limit = 2000
  cloud {
    enabled = false
    requestIdHeader = "X-REQUEST-ID"
  }

  ybc_flags {
    nfs_dirs = "/tmp/nfs,/nfs"
    enable_verbose = false
  }

  internal {
    # Enabling removes supported instance type filtering on AWS providers.
    allow_unsupported_instances = false
  }

  dbmem {
    postgres {
      max_mem_mb = 0
      # Max memory value for read replicas.
      # -1: use the same value as max_mem_mb
      # 0: Do not set read replica max mem
      # >0: Set read replica max mem mb to this value
      rr_max_mem_mb = -1
    }

    checks {
      # Used in checking memAvailable.
      mem_available_limit_kb = 716800
    }
  }

  backup {
    pg_based = false
    disable_xxhash_checksum = false
    log.verbose = false
    minIncrementalScheduleFrequencyInSecs = 900
  }

  logs {
    cmdOutputDelete = true
    max_msg_size = 2M
    shell.output_retention_hours = 1
    shell.output_dir_max_size = 10K
  }

  audit {
    log {
      # Used in AuditAction to enable audit logging checks
      verifyLogging = false
      outputToStdout = false
      outputToFile = true
      rolloverPattern = "yyyy-MM-dd"
      maxHistory = "30"
    }
  }

  snapshot_creation {
    # Config for attempts and delays for snapshot creation
    max_attempts = 80
    delay = 15
  }

  support_bundle {
    # default N days of logs to get if no dates specified
    default_date_range = 7
    application_logs_regex_pattern = "application-log-\\d{4}-\\d{2}-\\d{2}\\.gz"
    application_logs_sdf_pattern = "'application-log-'yyyy-MM-dd'.gz'"
    k8s_mount_point_prefix = "/mnt/disk"
    default_mount_point_prefix = "/mnt/d"
    universe_logs_regex_pattern = "((?:.*)(?:yb-)(?:master|tserver)(?:.*))(\\d{8})-(?:\\d*)\\.(?:.*)"
    postgres_logs_regex_pattern = "((?:.*)(?:postgresql)-)(.{10})(?:.*)"
    ybc_logs_regex_pattern = "((?:.*)(?:yb-)(?:controller)(?:.*))(\\d{8})-(?:\\d*)\\.(?:.*)"
    retention_days = 10
    k8s_enabled = true
    onprem_enabled = true
  }
  # certificate issued would be with expiry of following
  tlsCertificate {
    root.expiryInYears = 4
    server.maxLifetimeInYears = 1
  }

  # External script object used to store script details
  external_script {
    content = null
    params = null
    schedule = null
  }

  query_stats {
    excluded_queries = [
      "SET extra_float_digits = 3"
    ]
    slow_queries {
       limit = 50
       # Descending sort possible values: total_time, max_time, mean_time, rows, calls
       # columns of pg_stat_statements view
       # See https://www.postgresql.org/docs/current/pgstatstatements.html#id-1.11.7.39.6
       order_by = "total_time"
       set_enable_nestloop_off = true
    }

    # Different wait times for live queries
    # This is runtime configurable but will take effect only after restart.
    live_queries {
      ws = ${play.ws}
    }
    live_queries.ws.timeout.connection = 10 seconds
    live_queries.ws.timeout.idle = 30 seconds
    live_queries.ws.timeout.request = 30 seconds
    live_queries.ws.ssl.loose.acceptAnyCertificate = true

    # initial and minimum number of threads used by import controller
    core_threads = 1

    # max number of threads we will grow to if needed before starting to reject tasks
    max_threads = 50

    # duration for which thread pool will stay inflated before it shrinks back to core_threads
    thread_ttl = 1 minute

    # capacity of the thread pool queue
    queue_capacity = 500
  }

  # Allow http to https redirects for node UI
  node_ui {
    https {
      enabled = true
    }
    ws = ${play.ws}
  }
  node_ui.ws.ssl.loose.acceptAnyCertificate = true
  node_ui.ws.timeout.connection = 10 seconds
  node_ui.ws.timeout.idle = 30 seconds
  node_ui.ws.timeout.request = ${yb.proxy_endpoint_timeout}

  attach_detach {
    enabled = false
  }

  node_agent {
    connect_timeout = 10 seconds
    poller_interval = 2 minutes
    retention_duration = 30 days
    max_parallel_upgrades = 30

    client {
        enabled = false
    }

    server {
        install = false
    }

    live_node_poller {
        # initial and minimum number of threads used by live node poller
        core_threads = 10
        # max number of threads we will grow to if needed before starting to reject tasks
        max_threads = 50
        # duration for which thread pool will stay inflated before it shrinks back to core_threads
        thread_ttl = 1 minute
        # capacity of the thread pool queue
        queue_capacity = 1000
    }

    dead_node_poller {
        # initial and minimum number of threads used by dead node poller
        core_threads = 4
        # max number of threads we will grow to if needed before starting to reject tasks
        max_threads = 10
        # duration for which thread pool will stay inflated before it shrinks back to core_threads
        thread_ttl = 1 minute
        # capacity of the thread pool queue
        queue_capacity = 1000
    }

    ansible_offloading {
      enabled = false
      min_supported_version = "2.17.3.0"
    }

    preflight_checks {
      min_python_version = 2.7
      user = yugabyte
      user_group = yugabyte
      min_prometheus_space_mb = 100
      min_tmp_dir_space_mb = 100
      min_home_dir_space_mb = 100
      min_mount_point_dir_space_mb = 100
      ulimit_core = unlimited
      ulimit_open_files = 1048576
      ulimit_user_processes = 12000
      swappiness = 0
      ssh_timeout = 10
      vm_max_map_count = 262144
    }
    releases {
        # Path to the node-agent releases.
        path = "/opt/yugabyte/node-agent/releases"
    }
  }

  gflags {
    allow_user_override = false
  }

  helm {
    timeout_secs = 900
    packagePath = ""
  }

  features {
    cert_reload {
      enabled = true
      supportedVersions = ["2.14+"]
    }
  }

  kms {
    refresh_interval = 12 hours
  }

  api {
    backward_compatible_date = ${yb.cloud.enabled}
  }
}

ybc {
  releases {
    stable_version = "1.0.0-b26"
    path = "/opt/yugabyte/ybc/releases"
  }
  docker {
    release = "/opt/yugabyte/ybc/release"
  }
  upgrade {
    scheduler_interval = 2 minute,
    universe_batch_size = 5,
    node_batch_size = 15,
    allow_scheduled_upgrade = true
  }
  k8s {
    enabled = true
  }
  timeout {
    admin_operation_timeout_ms = 120000
    socket_read_timeout_ms = 30000
    operation_timeout_ms = 60000
  }
}

runtime_config {
  included_objects = [
    "yb.external_script"
    "yb.ha.ws"
    "yb.query_stats.live_queries.ws"
    "yb.alert.pagerduty.ws"
    "yb.alert.slack.ws"
    "yb.alert.webhook.ws"
    "yb.perf_advisor"
    "yb.runtime_conf_ui.tag_filter"
    "yb.universe.user_tags.enforced_tags"
  ]
  included_paths = [
    #  We can set this to "yb." if/when there are more includedPaths than excludedPaths
    "yb.taskGC."
    "yb.universe.auth.is_enforced"
    "yb.ui.enable_dedicated_nodes"
    "yb.alert.max_clock_skew_ms"
    "yb.wait_for_master_leader_timeout"
    "yb.customer_task_db_query_limit"
    "yb.proxy_endpoint_timeout"
    "yb.cloud.enabled" # should be excluded for cloud deployments
    "yb.universe_boot_script",
    "yb.ui.show_cost",
    "yb.ui.feature_flags."
    "yb.health.logOutput"
    "yb.health.nodeCheckTimeoutSec"
    "yb.health.max_num_parallel_node_checks"
    "yb.ha.logScriptOutput"
    "yb.internal.",
    "yb.ansible.",
    "yb.upgrade.",
    "yb.edit.",
    "yb.releases.",
    "yb.tls.skip_cert_validation",
    "yb.dbmem.",
    "yb.security.enable_detailed_logs",
    "yb.security.ldap.",
    "yb.use_kubectl",
    "yb.use_new_helm_naming",
    "yb.security.use_oauth",
    "yb.security.type",
    "yb.security.clientID",
    "yb.security.secret",
    "yb.security.discoveryURI",
    "yb.security.oidcScope",
    "yb.security.oidcEmailAttribute",
    "yb.security.ssh2_enabled"
    "yb.security.custom_hooks.enable_custom_hooks",
    "yb.security.custom_hooks.enable_sudo",
    "yb.query_stats.slow_queries.set_enable_nestloop_off",
    "yb.backup.pg_based",
    "yb.backup.disable_xxhash_checksum",
    "yb.logs.",
    "yb.audit.",
    "yb.metrics.db_read_write_test",
    "yb.metrics.collection_level",
    "yb.metrics.ui.topk.enable",
    "yb.support_bundle.k8s_enabled",
    "yb.support_bundle.onprem_enabled",
    "yb.internal.allow_unsupported_instances",
    "yb.universe_version_check_mode",
    "yb.task.override_force_universe_lock",
    "yb.query_stats.",
    "yb.runtime_conf_ui.enable_for_all",
    "yb.security.ssh_keys.enable_ssh_key_expiration",
    "yb.security.ssh_keys.ssh_key_expiration_threshold_days"
    "yb.is_platform_downgrade_allowed",
    "yb.ybc_flags.",
    "ybc.upgrade.",
    "ybc.releases.stable_version",
    "ybc.k8s."
    "yb.node_agent.preflight_checks.",
    "yb.gflags.",
    "yb.start_master_on_stop_node",
    "yb.health.trigger_api.enabled",
    "yb.backup.log.verbose",
    "yb.wait_for_lb_for_added_nodes",
    "yb.features.cert_reload.enabled",
    "yb.edit_provider.new.enabled",
    "yb.provider.skip_keypair_validation",
    "yb.fs_stateless.",
    "yb.kms.refresh_interval",
    "yb.snapshot_cleanup.delete_orphan_on_startup",
    "yb.helm.timeout_secs",
    "yb.perf_advisor.cleanup.rec_retention_duration",
    "yb.perf_advisor.cleanup.pa_run_retention_duration",
    "yb.node_ui.https.enabled",
    "yb.attach_detach.enabled",
    "yb.xcluster.transactional.enabled"
  ]
  excluded_paths = [
    # this ws config is in included_objects:
    "yb.query_stats.live_queries.ws.",
    "yb.query_stats.core_threads",
    "yb.query_stats.max_threads",
    "yb.query_stats.queue_capacity",
    "yb.query_stats.thread_ttl",
    "yb.perf_advisor.universe_batch_size",
    "yb.perf_advisor.scheduler_interval_mins",
    "yb.perf_advisor.cleanup.gc_check_interval".
    "yb.releases.path"
  ]
  scope_strictness.enabled = true
  data_validation.enabled = true
}

kamon.prometheus {
  embedded-server {
    hostname = "localhost"
    port = 9095
  }
}
